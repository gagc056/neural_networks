{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMaOTR2QcHNewFrRTw/PlRM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gagc056/neural_networks/blob/main/redes_neuronales_reg_clas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShhXpiI_dtER"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\n",
        "concrete_data.head()\n",
        "\n",
        "concrete_data.shape\n",
        "\n",
        "concrete_data.describe()\n",
        "concrete_data.isnull().sum()\n",
        "\n",
        "#Split data into predictors and target\n",
        "#The target variable in this problem is the concrete sample strength. \n",
        "#Therefore, our predictors will be all the other columns.\n",
        "concrete_data_columns = concrete_data.columns\n",
        "\n",
        "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
        "target = concrete_data['Strength'] # Strength column\n",
        "\n",
        "#Let's do a quick sanity check of the predictors and the target dataframes.\n",
        "predictors.head()\n",
        "target.head()\n",
        "\n",
        "#Finally, the last step is to normalize the data by substracting the mean and dividing\n",
        "#by the standard deviation.\n",
        "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
        "predictors_norm.head()\n",
        "\n",
        "#Let's save the number of predictors to _n_cols_ since we will need this number when building our network.\n",
        "n_cols = predictors_norm.shape[1] # number of predictors\n",
        "\n",
        "## Import Keras\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "#build a neural network\n",
        "# define regression model\n",
        "def regression_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    # compile model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Train and Test the Network\n",
        "# build the model\n",
        "model = regression_model()\n",
        "\n",
        "# fit the model\n",
        "model.fit(predictors_norm, target, validation_split=0.2, epochs=100, verbose=2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import the data\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# read the data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train.shape\n",
        "\n",
        "plt.imshow(X_train[0])\n",
        "\n",
        "#With conventional neural networks, we cannot feed in the image as input as is.\n",
        "# So we need to flatten the images into one-dimensional vectors, each of size 1 x (28 x 28) = 1 x 784.\n",
        "num_pixels = X_train.shape[1] * X_train.shape[2] # find size of one-dimensional vector\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32') # flatten training images\n",
        "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32') # flatten test images\n",
        "\n",
        "#Since pixel values can range from 0 to 255, let's normalize the vectors to be between 0 and 1.\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "#Finally, before we start building our model, remember that for classification we need to divide our target variable into categories. \n",
        "#We use the to_categorical function from the Keras Utilities package.\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "num_classes = y_test.shape[1]\n",
        "print(num_classes)\n",
        "\n",
        "# Build a Neural Network\n",
        "# define classification model\n",
        "def classification_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_pixels, activation='relu', input_shape=(num_pixels,)))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    \n",
        "    # compile model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "## Train and Test the Network\n",
        "# build the model\n",
        "model = classification_model()\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "#Let's print the accuracy and the corresponding error.\n",
        "print('Accuracy: {}% \\n Error: {}'.format(scores[1], 1 - scores[1]))   \n",
        "\n",
        "#Just running 10 epochs could actually take over 20 minutes. But enjoy the results as they are getting generated.\n",
        "\n",
        "#Sometimes, you cannot afford to retrain your model everytime you want to use it, especially if you are limited on \n",
        "#computational resources and training your model can take a long time. Therefore, with the Keras library, you can save your model after training. \n",
        "#To do that, we use the save method.\n",
        "model.save('classification_model.h5')\n",
        "\n",
        "\n",
        "#Since our model contains multidimensional arrays of data, then models are usually saved as .h5 files.\n",
        "\n",
        "#When you are ready to use your model again, you use the load_model function from keras.models.\n",
        "from keras.models import load_model\n",
        "pretrained_model = load_model(\"classification_model.h5\")\n",
        "â€‹\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h8ZOiaF5eXVG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}